---
title: "Cross Validation and Bootstrap"
author: "Ozan Aygun"
date: "6/18/2017"
output: 
   html_document:
        toc: true
        number_sections: true
        depth: 4
        theme: cerulean
        highlight: tango
        df_print: paged
---

# Introduction

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,results = "markup", fig.align = "center", fig.width = 5, fig.height = 4,message=FALSE,warning=FALSE)
```

**Here our goal is to study resampling methods Cross Validation and Bootstrap.** 

___

# Cross-Validation

## Leave-one-out Cross Validation (LOOCV)

```{r}
require(ISLR); require(boot)
```

We will ise **cv.glm** function from boot package. This function calculates the estimated K-fold cross-validation prediction error for generalized linear models.

**Usage:**

**cv.glm(data, glmfit, cost, K)**

- **data:** matrix or data.frame containing the data. The rows should be cases and the columns correspond to variables, one of which is the response.

- **glmfit:** An object of class "glm" containing the results of a generalized linear model fitted to data.

- **cost:** A function of two vector arguments specifying the **cost function for the cross-validation.** The first argument to cost should correspond to the observed responses and the second argument should correspond to the predicted or fitted responses from the generalized linear model. cost must return a non-negative scalar value. **The default is the average squared error function.**

- **K:** The number of groups into which the data should be split to estimate the cross-validation prediction error. The value of K must be such that all groups are of approximately equal size. If the supplied value of K does not satisfy this criterion then it will be set to the closest integer which does and a warning is generated specifying the value of K used. **The default is to set K equal to the number of observations in data which gives the usual leave-one-out cross-validation.**

**Mechanism under the hood:**

The data is divided randomly into K groups. For each group the generalized linear model is fit to data omitting that group, then the function cost is applied to the observed responses in the group that was omitted from the fit and the prediction made by the fitted models for those 

When K is the number of observations leave-one-out cross-validation is used and all the possible splits of the data are used. When K is less than the number of observations the K splits to be used are found by randomly partitioning the data into K groups of approximately equal size. 

**Value:**

The returned value is a list with the following components.

- call: The original call to cv.glm.
- K: The value of K used for the K-fold cross validation.
- delta: A vector of length two. The first component is the raw cross-validation estimate of prediction error. The second component is the adjusted cross-validation estimate. The adjustment is designed to compensate for the bias introduced by not using leave-one-out cross-validation.
- seed:	The value of .Random.seed when cv.glm was called.

We will use the Auto data set from ISLR package:

```{r}
plot(mpg ~ horsepower, data = Auto, pch = 19, col = "navy" )
```

As we expected, there is a decrease in mpg with increased horsepower.

If we don't specify any family to glm, by default it fits a linear model:

```{r}
glm.fit = glm(mpg ~ horsepower, data = Auto)
cv.glm(Auto, glmfit = glm.fit)$delta
```

We get the estimated CV MSE and adjusted (bias-corrected version) CV MSE using our linear model that is fitted for the relationship between mpg (outcome) and horsepower (predictor).

Let's increase the model complexity and perform cross validation again. We recall that the relationship between the predictor and the outcome looks quite non-linear. Therefore, we will try adding polynomial terms and this will increase model complexity:

```{r}
cv.error = NULL
degree = 1:5 # we test up to 5th degree polynomial
for(d in degree){
  #For each degree of polynomial, fit a new model
  glm.fit = glm(mpg ~ poly(horsepower, degree = d), data = Auto) 
  # Calculate the LOOCV error for each model fit
  cv.error[d] = cv.glm(Auto, glm.fit)$delta[2]
}
plot(degree, cv.error, type = "b", col= "navy", pch = 20 )
```

As we can notice, addition of a second degree polynomial redices the CV error, and adding further terms do not seem to influence the error level.

## K- fold cross-validation

Let's try a 10-fold CV for the same problem. Note that in this case it is less work for the cv.glm function, because the model needs to be only fitted 10 times.

```{r}
cv.error10 = NULL
degree = 1:5 # we test up to 5th degree polynomial
for(d in degree){
  #For each degree of polynomial, fit a new model
  glm.fit = glm(mpg ~ poly(horsepower, degree = d), data = Auto) 
  # Calculate the LOOCV error for each model fit
  cv.error10[d] = cv.glm(Auto, glm.fit, K = 10)$delta[2]
}

plot(degree, cv.error10, type = "b", col= "red" )
```

In this example, LOOCV and 10-fold CV gave us pretty much the same answer. However, in most cases we favor a 10-fold CV because it provides us a more stable estimate of error and it is also much cheaper to compute.

# Bootstrap

Bootstrap is extremely useful for getting estimates of standard errors associated with **nasty statistics**. In other words, bootstrap lets us to get a sampling distribution for the statistics for which it is otherwise very hard to get a theoretical distribution. Very easy way of statistics, when the theory is very hard.

In this example, we have an **alpha** statistic which represents the minimum risk associated with two investments. 

Picking a combination of two investments is represented by a quite non-linear formula.

We have two investments:

- X and Y

The risk (volatility) associated with them represented by:

- VAR(X) and VAR(Y)

The optimal formula for getting minimum risk for these two investments is given by the formula:

___

alpha = [VAR(Y) - COV(X,Y)] / [VAR(X)+VAR(Y) - 2 * COV(X,Y)]

___

Great! This is the formula of a new statistic, which we can directly compute if we know the values of X and Y.

**But, how can we know the variability of alpha? We don't know its theoretical distribution, and it looks very non-linear. How can we draw inferences about alpha if we have given a sample of alphas for comparison?**

This is where bootstrap really becomes handy.

Let's first define a function that can compute alpha in R:

```{r}
alpha <- function(x,y){
        # y and y are two vectors of actual data
 (var(y) - cov(x,y))/(var(x)+ var(y) - 2 * cov(x,y))  
}
```

We will use the data set Portfolio to calculate alpha:

```{r}
alpha(Portfolio$X,Portfolio$Y)
```

Now the question is: **what is the standard error of alpha?**

We have no a priori distribution to calculate the standard error, therefore we will use bootsrap to estimate this standard error associated this unusual statistic:

To make our function usable for bootstrap, we slightly modify it:
```{r}
alpha.fn <- function(data, index){
        # data: is a given data frame that contains the vectors
        # index: row indexes of the data frame to be used. This will allow us to change the row indexes that will be used in the calculation of the statistic in repeated samples (see below) 
        # with function tells that use a data frame (or portion of it like in this case), and calculate the value of the function, where arguments can be directly passed in their original names as in the data frame
        with(data = data[index,],alpha(X,Y))
}

```

Let's try it by using all rows in the data:

```{r}
alpha.fn(Portfolio,1:100)
```

Now we can try bootstrap: what we are doing is, we are getting the all rows indexes from the data frame, get a sample of exact size (100 in this case), but **with replacement**. We then compute alpha by using these new **bootstrap samples from the original data**:

```{r}
set.seed(1) # Need to set.seed for reproducibility
alpha.fn(Portfolio,sample(1:100,100,replace = TRUE))
```

This is one **bootstrap estimate** of alpha. Note that it is not the same as the original sample estimate, but close. 

Next, if we keep doing this bootstrap sampling and compute alpha many many times, we can get a frequency distribution (sampling distribution) of the alphas. 

We will use the **boot** function to perform this:

note that the first 3 arguments are critical:

- data: data frame used
- statistic: the formula for the statistic to be computed
- R: number of bootstrap repeats performed

```{r}
boot.out <- boot(Portfolio,alpha.fn, R = 1000)
boot.out
```

That's it! Note that we got the original estimate of our statistic, bias associated with the bootstrap, and our **bootstrap standard error** estimate as the outputs.

boot function also returns nice plots of the bootstrap distribution:

```{r, fig.width= 10, fig.height=6}
plot(boot.out)
```

Nice! It prints us out the bootstrap distribution, the dashed line marks the position of our estimate. Q-Q plot also confirms that the distribution we obtained is **near-normal distribution**.

# Example problem: prediction using Default data set

## Testing(validation set) approach

Eariler we used logistic regression to predict the probability of default using income and balance on the Default data set. We will now estimate the test error of this logistic regression model using the validation set approach.

```{r}
data("Default")
head(Default)
set.seed(1)
library(scales)
with(Default, plot(income,balance, pch = 20, col = scales::alpha( ifelse(default == "Yes", "navy","pink"),0.7)))
```

Fit a logistic regression model that uses income and balance to predict default.

```{r}
glm.default <- glm( default ~ income + balance, data = Default, family = "binomial")
```

Using the validation set approach, estimate the test error of this model. In order to do this, you must perform the following steps:

i.Split the sample set into a training set and a validation set:

```{r}
library(caret)
Intrain <- createDataPartition(Default$default, p = 0.7, list = FALSE)
train.set <- Default[Intrain,]
test.set <- Default[-Intrain,]
```

ii. Fit a multiple logistic regression model using only the training observations.

```{r}
glm.train <- glm(default ~ income + balance, data = train.set, family = "binomial")
```

iii. Obtain a prediction of default status for each individual in the validation set by computing the posterior probability of default for that individual, and classifying the individual to the default category if the posterior probability is greater than 0.5.

```{r}
# Vector of the fitted probabilities
prob.fitted <- predict(glm.train,test.set)
# Classifications
predictions1 <- ifelse(prob.fitted > 0.5,"Yes", "No")
table(predictions1,test.set$default)
```

Compute the validation set error, which is the fraction of the observations in the validation set that are misclassified.

```{r}
mean(predictions1 != test.set$default)
```

Our error rate is about 2.6 %.

Repeat the process in (b) three times, using three different splits of the observations into a training set and a validation set. Comment on the results obtained.

```{r}
errors <-mean(predictions1 != test.set$default)
for (i in 2:4){
Intrain <- createDataPartition(Default$default, p = 0.7, list = FALSE)
train.set <- Default[Intrain,]
test.set <- Default[-Intrain,] 
glm.train <- glm(default ~ income + balance, data = train.set, family = "binomial")   
# Vector of the fitted probabilities
prob.fitted <- predict(glm.train,test.set)
# Classifications
temp <- ifelse(prob.fitted > 0.5,"Yes", "No")
errors[i] <- mean(temp != test.set$default)

}
errors
```

In all 4 attempts, the error rate is similar, but not exactly the same. The small variability exists due to random sampling to form the validation set.

___

Now consider a logistic regression model that predicts the prob- ability of default using **income, balance, and a dummy variable for student**. Estimate the test error for this model using the validation set approach. Comment on whether or not including a dummy variable for student leads to a reduction in the test error rate.

Develop some expectations:

```{r, fig.height=6, fig.width=9}
par(mfrow = c(1,2))
with(Default[Default$student == "Yes",],plot(income,balance, pch = 20, col = scales::alpha( ifelse(default == "Yes", "navy","pink"),0.7), main = "Students"))
with(Default[Default$student == "No",],plot(income,balance, pch = 20, col = scales::alpha( ifelse(default == "Yes", "navy","pink"),0.7), main = "Non-Students"))
```

We don't see an obvious difference between the two groups interms of our classification of interest. In general, balance seems to be the best predictor amongst all.

Let's first add the dummy variable:

```{r}
Default$student.binary <- factor(ifelse(Default$student == "Yes", 1,0))
```

Then, re-fit the model using validation set approach:

```{r}
set.seed(1)
errors.student <-NULL
for (i in 1:4){
Intrain <- createDataPartition(Default$default, p = 0.7, list = FALSE)
train.set <- Default[Intrain,]
test.set <- Default[-Intrain,] 
glm.train <- glm(default ~ income + balance + student.binary, data = train.set, family = "binomial")   
# Vector of the fitted probabilities
prob.fitted <- predict(glm.train,test.set)
# Classifications
temp <- ifelse(prob.fitted > 0.5,"Yes", "No")
errors.student[i] <- mean(temp != test.set$default)

}
errors
errors.student
```

Note that our error rates inflated in general, while in certain attempts we got similar error values. This could be because we have added a predictor that is not useful for the classification. By doing this, we increased model complexity but the amount of the data that is used to estimate the increased number of coefficients remained the same. While the bias is reduced, the variability increased, since we have more parameters to estimate with the same amount of data. This makes model less stable, causes overfitting, and we diverge from the Bayesian error rate.

## The impact of the size of the training set

What happens if we have different size of trainins sets? Let's test this impact, using 50, 70 and 80 percent of the data for training, and remaining portions for testing:


```{r}
training.portions <- c(0.5,0.7,0.8)
final.results <- NULL
for(j in seq_along(training.portions)){
set.seed(1)
results <- data.frame(training.portions = 1:100, error = 1:100,
                      student.included = 1:100)
        for (i in 1:100){
        
        Intrain <- createDataPartition(Default$default, p = 0.7, list = FALSE)
        train.set <- Default[Intrain,]
        test.set <- Default[-Intrain,] 
        glm.train <- glm(default ~ income + balance, data = train.set, family = "binomial")   
        # Vector of the fitted probabilities
        prob.fitted <- predict(glm.train,test.set)
        # Classifications
        temp <- ifelse(prob.fitted > 0.5,"Yes", "No")
        results$error[i] <- mean(temp != test.set$default)
        results$training.portions[i] = training.portions[j]
        results$student.included[i] = "No"
 }
 final.results <- rbind(final.results,results)
}

# Model including the student dummy variable

for(j in seq_along(training.portions)){
set.seed(1)
results <- data.frame(training.portions = 1:100, error = 1:100,
                      student.included = 1:100)
        for (i in 1:100){
        
        Intrain <- createDataPartition(Default$default, p = 0.7, list = FALSE)
        train.set <- Default[Intrain,]
        test.set <- Default[-Intrain,] 
        glm.train <- glm(default ~ income + balance + student.binary, data = train.set, family = "binomial")   
        # Vector of the fitted probabilities
        prob.fitted <- predict(glm.train,test.set)
        # Classifications
        temp <- ifelse(prob.fitted > 0.5,"Yes", "No")
        results$error[i] <- mean(temp != test.set$default)
        results$training.portions[i] = training.portions[j]
        results$student.included[i] = "Yes"
 }
 final.results <- rbind(final.results,results)
}

library(ggplot2)
ggplot(aes(x = factor(training.portions), y = error), data = final.results) +geom_boxplot()+ geom_jitter()+ facet_grid( . ~ student.included) +theme_bw() 
```

```{r}
library(dplyr)
final.results %>% group_by(student.included,training.portions) %>% summarise(mean.error = mean(error), sd.error = sd(error))
```

We notice that in this example if we perform the sampling many many times, the mean error converges to the same value in both models.

```{r}
ggplot(aes(x =  error, group = student.included , color = student.included), data = final.results) +geom_density() +theme_bw() +scale_colour_discrete() +facet_grid
```

The distribution of the errors for different sampling proportions are very similar, however they are slightly differet between the two models as we expected.


```
```