---
title: "Logistic Regression Tutorial"
author: "Ozan Aygun"
date: "6/04/2017"
output: 
   html_document:
        toc: true
        number_sections: true
        depth: 4
        theme: cerulean
        highlight: tango
        df_print: paged
---

# Introduction

**Here our goal is to use train a simple logistic regression model for making predicitons.** 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,results = "markup", fig.align = "center", fig.width = 5, fig.height = 4,message=FALSE,warning=FALSE)
```

___

Some facts about logistic regression:

- Powerful generalization of a linear model, utilizes **logit** link function and **binomial** family distribution to predict **log ODDS** of a particular response.
- Model coefficients (parameters) are estimated by maximizing the **Maximum Likelihood** from the training data.
- Logistic regression is quite popular for classification, especially when K = 2 (when number of response classes is 2)

___

In this exercise, we will use the Smarket data set from ISLR package. This data represents daily percentage returns for the S&P 500 stock index between 2001 and 2005.

___

#Basic exploration of the data

```{r,fig.width = 10, fig.height = 10}
require(ISLR)
names(Smarket)
str(Smarket)
summary(Smarket)
library(knitr)
knitr::kable(head(Smarket),align = "c")
plot(Smarket[,-9], col = ifelse(Smarket$Direction == "Up","green","red"), cex = 0.5, pch = 20)
```

We are going to predict the direction of the market (up or down) given the features available. Note that the response variable Direction is derived from the Today variable, therefore we will not include this variable in our model.

# Fitting the Logistic regression model

```{r}
glm.fit <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4+ Lag5+ Volume, data = Smarket, family = "binomial")
summary(glm.fit)
```

Note that none of the features appear significant from this model. Not surprising for the stock market data, it also looked fairly noisy without much correlation with the response.

**This doesn't mean we can not make predictions by using this model!**

# Making predictions from a logistic regression model:

Using predict function to get probabilities using the training data we used to fit the model:

```{r}
glm.probs <- predict(glm.fit,type = "response")
glm.probs[1:5] # Vector of the fitted probabilities
```

Note that all the predicted probabilities are close to 50%, another indication that we won't be able to make strong predictions using this model.

# Turning probabilities into classifications

```{r}
glm.pred <- ifelse(glm.probs > 0.5, "Up", "Down")
```

# Prepare confusionMatrix

- Diagonal: the values our model got right.
- Off-diagonal: the values our model missed.

```{r}
knitr::kable(table(glm.pred,Smarket$Direction), align = "c")
```

# Calculating the mean performance of the model

Just calculate, on average, how many times the model got the response class correctly:

```{r}
mean(glm.pred == Smarket$Direction)
```

It is a little better than random guessing! Therefore, there is lots of room for improvement for the model, or we need better data or features to make stronger predictions. Note that this accuracy will be even less in the test data!

# Refining the model using fever predictors

What happens if we had used a smaller model?

```{r}
glm.fit2 <- glm(Direction ~ Lag1 + Lag2, data = Smarket, family = "binomial")
summary(glm.fit2)
glm.probs2 <- predict(glm.fit2, type = "response")
glm.pred2 <- ifelse(glm.probs2 > 0.5, "Up", "Down")
mean(glm.pred2 == Smarket$Direction)
```

Note that the mean accuracy get slightly better at least in the training data set. Therefore, we might be slightly overfitting the data by using all predictors.
