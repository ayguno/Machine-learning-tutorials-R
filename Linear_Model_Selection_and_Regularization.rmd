---
title: "Linear model selection and Regularization"
author: "Ozan Aygun"
date: "6/25/2017"
output: 
   html_document:
        toc: true
        number_sections: true
        depth: 4
        theme: cerulean
        highlight: tango
        df_print: paged
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,results = "markup", fig.align = "center", fig.width = 6, fig.height = 6,message=FALSE,warning=FALSE)
```


# Introduction

In this tutorial our goal is to understand what we can do to go beyond the ordinary least squares estimation of coefficients, while still staying in the linear regression framework.

# Why do we want to go beyond ordinary least squares (OLS) regression ?

- Let's say we have a LONG data set: 1 millon observations and 3-4 predictors (i.e: n >> p) in this case it is very tempting to use OLS regression, as well as more flexible approaches such as quadratic regression, Tree-based models or Support Vector Machines.

- But what happens when we have a WIDE data set? For instance 500 observations and 300 predictors, or 10,000 predictors, such as in genomics and proteomics data? (i.e: p >> n ). In such cases, OLS regression will severely suffer from overfitting if we include all these features together. The coefficients will be unstable, their standard errors will be large since we are trying to estimate so many parameters with very few data. 

Therefore, when p > n, or even when p ~ n, we need altertative approaches to:

1. **Increase prediction accuracy**: by reducing the variance of the coefficients and making the model more stable (i.e: reduce overfitting)

2. **Making the model more interpretable**: this relates to FEATURE SELECTION paradigm. Using certain rationalle, we come up with a smaller set of predictors (for instance by shrinking their coefficients to zero). As the model gets less complex, it becomes easier to interpret.

# Three classes of methods 

There are at least 3 orthagonal classes of methods we can apply to achieve these improvements beyond the OLS:

##Subset selection methods

This involves identifiying a small subset of predictors that we think closest related to the response. We then use this reduced set of variables to fit the model. Examples of this approach are:

- Best subset selection
- Forward selection
- Reverse selection

After we perform these strategies, we still need to estimate the test error of various different models being generated during the process. This forms the basis of selecting the **best model**. In general, once any of these selection processes are applied, we pick the best model by using either of these two approaches:

- Adjustment-based methods:

i) adjust the training error to provide an estimate of the test error: these involves calculating and comparing metrics like Cp/AIC or BIC. Pick the model that has the smallest Cp/AIC or BIC.

ii) Adjust the R-squared for each model (doesn't work for glms): unlike R-squared (which always increase as the model gets more complex, therefore misleading), adjusted R-squared **pays the price** for adding unnecessary variables in the model, therefore it is a more legitimate measure of optimal model selection.


- Cross-validation: more prefered approach. Perform cross-validation for each step of model selection and record the cross-validation error. FInally pick the model that gives the least cross-validation error.

## Regularization (Shrinkage Methods, Ridge and Lasso regression)

In this orthagonal approach, we try to simplify the model by penalizing the coefficients as they increase. Therefore, the coefficients are shrunken towards zero.

This shrinkage is known as **regularization**. 

The main advantage of regularization is that it **reduces the variance of coefficients and tries to make the model more stable**. In the case of lasso, it can also perform model selection.

In both Ridge and Lasso regression, the tuning parameter, **lambda** optimizes the model.

The calculations that are optimized for either of these models are very similar:

___


For Ridge (also called L2 regularization):

**RSS + lambda * sum([ Bj ]^2)**

For Lasso (also called L1 regularization):

**RSS + lambda * sum( abs( Bj ))**

L1: abs( Bj ) (sum of the absolute values of B's)

___


These calculations are also refered to as **convex optimizations.** Note that the choice of lambda plays a critical role on how the coefficients for each predictor (Bj) will be determined.

When lambda = 0, the convex optimization is simply RSS, in other words, the parameters will be obtained by using the ordinary least squares.

As lambda increases, it would be harder to minimize the comvex optimization, therefore the coefficients for each predictor (Bj) will be penalized and start to shriking towards the zero.

Due to the nature of the computations, in Ridge regression coefficients usually never exactly becomes zero even though they reach very close to zero. In Lasso regression however, certain coefficients will shrink exactly to zero as lambda increases. Therefore, Lasso also performs feature selection. For this reason, Lasso is generally useful over Ridge, when the desired model is more sparse (i.e: requires less number of features/complexity).

# Ridge and Lasso regression example

We will use **glmnet** package to implement Ridge and Lasso regularization. glmnet doesn't use the formula interface, we need to define predictors and outcome in a different way. This involves using the **model.matrix()** function.

model.matrix() creates a design (or model) matrix, e.g., by expanding factors to a set of dummary variables (depending on the contrasts) and expanding interactions similarly.

We will use the Hitters data set from the ISLR package, where we would like to fit a model to predict the salary of baseball players using their various game statistics as predictors. 

```{r}
library(glmnet)
library(ISLR)
data("Hitters")

# We will remove missing values for this example
Hitters <- na.omit(Hitters)

# We create a matrix of predictors
x = model.matrix(Salary ~ . -1, data = Hitters) 
# Note we use all predictors Salary ~ .
# Note we add -1 since we don't want an intercept to appear
# Note model.matrix drops the outcome vector, and it converts the factor variables into dummy variables 


# We create a response vector
y = Hitters$Salary
```

# Ridge regression

Ridge regression is fitted by calling glmnet with alpha = 0 argument. alpha is the elasticnet mixing parameter, with 0≤α≤ 1.

alpha=1 is the lasso penalty, and alpha=0 the ridge penalty. 

For alpha between 0 and 1 you will get elasticnet models.

```{r,fig.height= 6}
fit.ridge <- glmnet(x, y, alpha = 0)
plot(fit.ridge,xvar = "lambda", label = TRUE)
```

We can see how coefficients are penalized as lambda increases.

Now, we need to pick an optimal value of lambda to pick the best model coefficients. Note that there is also **cv.glmnet** function which can perform k-fold cross-validation.

```{r, fig.height= 6}
# Note that default is 10-fold cross-validation
cv.ridge <- cv.glmnet(x,y,alpha = 0)
plot(cv.ridge)
```

Notice that what we are doing here is actually computatonally intense. 

1. We get very fine grids of labda values (we can customize the range of values, otherwise function will define one for us)
2. Using each lambda value, we perform 10-fold cross validation:
- We fit a model using the Sum of Squares convex optimization and obtain cross-validated error (average of 10 model fits)
3. We continue steps 1 and 2 for all fine grids of lambda values in the range.

Finally we can come up with a series of CV errors (MSE in this case since the outcome is continuous) for all fine grid of values of lambda.

The object returned by cv.ridge has a builtin plot where we can observe the Mean Square Error as a function of different values of log(lambda). 

In this example, we notice that cv.MSE is high when lambda is very high, when the model is restricted. When we relax lambda, cv.MSE gets lower and reaches to a minimum. 

The two vertical lines are produced in the plot, the left one marks the model with min MSE and the middle one shows a little more restricted model 1 standard error away from the minimum MSE. 

The numbers at the top of the plot indicate how many predictors remain in the model during the regularization. Note that since we are doing Ridge regression all predictors remained in the model (19 variables + intercept).

In this example, all models between the log(lambda) ~ 3  and log(lambda) ~ 6 appear to have the minimum MSE. When choosing the optimal model, we might prefer to get the more restricted model that is indicated by the second vertical line, where log(lambda) is closer to 8.

# Lasso regression

Recall that the lasso is very similar to ridge, except we are penalizing the abolute values of coefficients.

Note that in this case we need to define alpha =1, which can be omitted since this is the default of the glmnet function:

```{r, fig.height = 6}
# Fit the lasso regression model

fit.lasso <- glmnet(x,y)
plot(fit.lasso, xvar = "lambda", label = TRUE)
```

Note that we have zero variables in the case of lasso, because it is doing both shrinkage and variable selection.

---

Another intresting plot that can be produced, which plots the **% deviance explained (~ similar to: R-squared)** by the model:

```{r, fig.height= 6}
plot(fit.lasso, xvar = "dev", label = TRUE)
```

**This is a very useful plot**, especially in this example it reveals something quite interesting. 

Notice that the regularized model with shrunken coefficients can already explain 40% of the deviance when we have 5 predictors left and they are very close to zero. Strikingly, at the point when we have 12 predictors included in the model, we only added up another 10% deviance explained, but coefficients have grown substantially. This is the point where model got unstable and most likely started overfitting.

Take home lesson for us is to look carefully to this type of plot and try to locate the sweetspot where coefficients are shrunken, model is simplified, yet we are still able to explain a good fraction of the devience. That is likely to be the most stable model that win the bias-variance trade off.


---

Next, we can use cross-validation to pick the optimal model based on the CV.MSE:

```{r}
cv.lasso <- cv.glmnet(x,y)
plot(cv.lasso)
```

Great! Note what we find:

- The minimum cv.error is obtained when there are 15 predictors.

- 1 standard error away from this min cv.MSE corresponds to the point where we have 5 predictors, consistent with what we have seen above.


**Obtaining the coefficients**

We will use **coef** function to extract coefficients form the model objects.

Note that fit.lasso will have the entire path of coefficients, while cv.lasso will provide the optimal number of coefficients obtained after the cross-validation.

```{r}
coef(cv.lasso)
```

Note that cv.lasso picked the best model and provided all non-zero coefficients for the selected predictors to be included in the model as well as one model intercept.

___

## Making predictions

We can use **predict** function to make predictions using the glmnet model object.

Suppose we wante to get the MSE of the training error from our lasso model:

```{r}
fit.lasso
pred <- predict(fit.lasso,x)
dim(pred)
length(y)
```

Note that prediction is a matrix, each column represents a model prediction vector, obtained from a distinct selection of lambda, for each of the observations.

Obtain RMSE from each model prediction:

```{r}
rmse <- sqrt(apply((y-pred)^2,2,mean))
plot(log(fit.lasso$lambda),rmse, type = "b", col = "navy")

# The smallest test rmse we obtained:

order(rmse)[1]

# Learn the best lambda: the lambda that yields the model that gives the minimum rmse:

best.lambda <- fit.lasso$lambda[order(rmse)][1]
best.lambda

# Get the model coefficients that are found by the best lambda:
# Note the s = argument refers to lambda
coef(fit.lasso, s = best.lambda)
```

Using the model from cross-validation lasso:

Note that in this case we choose a lambda to get a single set of predictions. The default is to get the 1 standard error lambda, which is the lambda that gives the MSE that is 1 standard error higher than minimum cross-validated mse. This way we choose a more restricted model:

```{r}
cv.pred<- predict.cv.glmnet(cv.lasso,newx = x, s = "lambda.1se")
rmse.cv = sqrt(mean((y-cv.pred) ^2))
rmse.cv
```

# Principal components regression (PCR)

We will use the **pcr** function in the **pls** library to perform PCR. Once again we are focusing on the Hitters data set and try to estimate the Salary by using other predictors in the data set. Note that we are still workign with the complete cases of the data set where the missing values are omitted.

pcr function also uses the formula interface so it si relatively straightforward to use:

```{r}
library(pls)
set.seed(2)
pcr.fit <- pcr(Salary ~ . , data = Hitters, scale = TRUE, validation = "CV")
```

Note that here we set:

- scale = TRUE: to standardize each variable (i.e: divide each predictor with its standard error). This is performed before calculating the Principal Components to ensure that the specific scale in which the predictors are measured will not have ane effect.

- validation = "CV" : we ask the pcr function to compute a 10-fold cross-validation error for each possible value of the principal components (M) that can be derived from the data matrix.

summary function is used to visualize the resulting model.fit object:

```{r}
summary(pcr.fit)
```

This output pretty much explains what is going on when we perform the PCR:

**In the first section of the summary table we get the CV-estimated error:**

- RMSEP: Note that pcr returns the root mean squared error (to get MSE, just square the RMSE values). For each number of components used in the model, the predicted RMSE from 10-fold CV is provided. 0-component model corresponds to the NULL model which just contains an intercept.

We can use the **validationplot()** function to plot the cross-valitation estimated RMSE or MSE scores:

```{r}
validationplot(pcr.fit,val.type = "RMSEP")
validationplot(pcr.fit,val.type = "MSEP")
```

We note that min CV-error is obtained when we have M = 16 principal components used in the regression. Since this is barely higher than M =19 (number of entire set of predictors), using these many PCs in the model is not likely to provide any benefit.

Neverthless, we also note that when we use a single component (the first component), the CV-error is only slightly larget than this minimum, suggesting that using smaller number of components (1-2) might be sufficient to get the same performance. The test error could be even better in the case of less number of components since we might be overfitting as we approach to OLS.

**The second part of the summary table focuses on the variance-explained by using each model containing the indicated number of principal components:**

We can think this as the amount of information about the predictors or response that is captured by using M principal components. 

As an example, using only 1 Principal component captures about 38% of the variance, while adding another component will capture 60% of the total variance. Note that in this case adding 13 principal components are able to explain 99% of the total variance. Using all (M = p = 19) principal components will result in 100% variance explained.

This can be indirectly visualized by the **validationplot()** function by following the R-squared:

```{r}
validationplot(pcr.fit, val.type = "R2")
```

Again we note that addition of principal components beyond the 3-4 PCs do not substantially increase the overall model R-squared.

Let's say we would like to compute the MSE for the training data set we have, using the M = 5 principal components:

```{r}
pcr.pred <- predict(pcr.fit,newdata = Hitters,ncomp = 5)
# Note that we get a single vector of predictions since we only used a single model using 5 principal components

# calculate MSE:
mean( (y - pcr.pred)^ 2)

# compare with the MSE we obtained with lasso:

rmse.cv ^2
```

Note that the MSE we obtained with PCR is competitive (or event lower than) the MSE we obtained from the lasso regression. 

Here the choice of the model depends on the application. 
- If our ultimate goal is to make predictions with miminum possible MSE, we would use PCR. 

- If we seek parsimony, i.e: a more interpretable model, we may want to go with the lasso, since the coefficients are produced for the original set of predictors, therefore making the resulting model more interpretable.

# Partial Least Squares (PLS) Regression

PCR is mainly converned explaining the variance within the predictors, and assumes that this is sufficent to explain the variance in the response, since often response will vary in the ame direction as the predictors.

While this is often the case, there are situations where the variation in the response can not be faithfully captured just by explaining the variability in the predictors.

Partial Least Squares provides an alternative to PCR, by taking the direction of response into account when fitting the model.

PLS is implemented using the **plsr** function within the pls library:

```{r}
set.seed(1)
pls.fit <- plsr(Salary ~ . , data = Hitters, scale = TRUE, validation = "CV" )
summary(pls.fit)
validationplot(pls.fit, val.type = "MSEP")
```

From the second part of the summary table, we can now notice that PLS, (in contrast to PCR), also tries to explain the variance in the response, Salary.


We again note that going beyond the 5th component have marginal impact. Therefore, let's make predictions using the model with 5 PCs:

```{r}
pls.pred <- predict(pls.fit, newdata = Hitters, ncomp = 5)

# Calculate MSE:
mean((y - pls.pred)^2)

# compare MSEs for all models we used:

MSE.table <- data.frame(lasso = (rmse.cv ^2),
                        pcr = mean((y- pcr.pred)^2),
                        pls = mean((y- pls.pred)^2))

MSE.table
```

Note that in this case PLS provided the lowest MSE using the training data set.

A better comparison would be using an independent test or validation set, if available. 