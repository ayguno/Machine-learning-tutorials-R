---
title: "Linear model selection and Regularization"
author: "Ozan Aygun"
date: "6/25/2017"
output: 
   html_document:
        toc: true
        number_sections: true
        depth: 4
        theme: cerulean
        highlight: tango
        df_print: paged
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,results = "markup", fig.align = "center", fig.width = 6, fig.height = 6,message=FALSE,warning=FALSE)
```


# Introduction

In this tutorial our goal is to understand what we can do to go beyond the ordinary least squares estimation of coefficients, while still staying in the linear regression framework.

# Why do we want to go beyond ordinary least squares (OLS) regression ?

- Let's say we have a LONG data set: 1 millon observations and 3-4 predictors (i.e: n >> p) in this case it is very tempting to use OLS regression, as well as more flexible approaches such as quadratic regression, Tree-based models or Support Vector Machines.

- But what happens when we have a WIDE data set? For instance 500 observations and 300 predictors, or 10,000 predictors, such as in genomics and proteomics data? (i.e: p >> n ). In such cases, OLS regression will severely suffer from overfitting if we include all these features together. The coefficients will be unstable, their standard errors will be large since we are trying to estimate so many parameters with very few data. 

Therefore, when p > n, or even when p ~ n, we need altertative approaches to:

1. **Increase prediction accuracy**: by reducing the variance of the coefficients and making the model more stable (i.e: reduce overfitting)

2. **Making the model more interpretable**: this relates to FEATURE SELECTION paradigm. Using certain rationalle, we come up with a smaller set of predictors (for instance by shrinking their coefficients to zero). As the model gets less complex, it becomes easier to interpret.

# Three classes of methods 

There are at least 3 orthagonal classes of methods we can apply to achieve these improvements beyond the OLS:

##Subset selection methods

This involves identifiying a small subset of predictors that we think closest related to the response. We then use this reduced set of variables to fit the model. Examples of this approach are:

- Best subset selection
- Forward selection
- Reverse selection

After we perform these strategies, we still need to estimate the test error of various different models being generated during the process. This forms the basis of selecting the **best model**. In general, once any of these selection processes are applied, we pick the best model by using either of these two approaches:

- Adjustment-based methods:

i) adjust the training error to provide an estimate of the test error: these involves calculating and comparing metrics like Cp/AIC or BIC. Pick the model that has the smallest Cp/AIC or BIC.

ii) Adjust the R-squared for each model (doesn't work for glms): unlike R-squared (which always increase as the model gets more complex, therefore misleading), adjusted R-squared **pays the price** for adding unnecessary variables in the model, therefore it is a more legitimate measure of optimal model selection.


- Cross-validation: more prefered approach. Perform cross-validation for each step of model selection and record the cross-validation error. FInally pick the model that gives the least cross-validation error.

## Regularization (Shrinkage Methods, Ridge and Lasso regression)

In this orthagonal approach, we try to simplify the model by penalizing the coefficients as they increase. Therefore, the coefficients are shrunken towards zero.

This shrinkage is known as **regularization**. 

The main advantage of regularization is that it **reduces the variance of coefficients and tries to make the model more stable**. In the case of lasso, it can also perform model selection.

In both Ridge and Lasso regression, the tuning parameter, **lambda** optimizes the model.

The calculations that are optimized for either of these models are very similar:

___


For Ridge (also called L2 regularization):

**RSS + lambda * sum([ Bj ]^2)**

For Lasso (also called L1 regularization):

**RSS + lambda * sum( abs( Bj ))**

L1: abs( Bj ) (sum of the absolute values of B's)

___


These calculations are also refered to as **convex optimizations.** Note that the choice of lambda plays a critical role on how the coefficients for each predictor (Bj) will be determined.

When lambda = 0, the convex optimization is simply RSS, in other words, the parameters will be obtained by using the ordinary least squares.

As lambda increases, it would be harder to minimize the comvex optimization, therefore the coefficients for each predictor (Bj) will be penalized and start to shriking towards the zero.

Due to the nature of the computations, in Ridge regression coefficients usually never exactly becomes zero even though they reach very close to zero. In Lasso regression however, certain coefficients will shrink exactly to zero as lambda increases. Therefore, Lasso also performs feature selection. For this reason, Lasso is generally useful over Ridge, when the desired model is more sparse (i.e: requires less number of features/complexity).

# Ridge and Lasso regression example

We will use **glmnet** package to implement Ridge and Lasso regularization. glmnet doesn't use the formula interface, we need to define predictors and outcome in a different way. This involves using the **model.matrix()** function.

model.matrix() creates a design (or model) matrix, e.g., by expanding factors to a set of dummary variables (depending on the contrasts) and expanding interactions similarly.

We will use the Hitters data set from the ISLR package, where we would like to fit a model to predict the salary of baseball players using their various game statistics as predictors. 

```{r}
library(glmnet)
library(ISLR)
data("Hitters")

# We will remove missing values for this example
Hitters <- na.omit(Hitters)

# We create a matrix of predictors
x = model.matrix(Salary ~ . -1, data = Hitters) 
# Note we use all predictors Salary ~ .
# Note we add -1 since we don't want an intercept to appear
# Note model.matrix drops the outcome vector, and it converts the factor variables into dummy variables 


# We create a response vector
y = Hitters$Salary
```

# Ridge regression

Ridge regression is fitted by calling glmnet with alpha = 0 argument. alpha is the elasticnet mixing parameter, with 0≤α≤ 1.

alpha=1 is the lasso penalty, and alpha=0 the ridge penalty. 

For alpha between 0 and 1 you will get elasticnet models.

```{r,fig.height= 6}
fit.ridge <- glmnet(x, y, alpha = 0)
plot(fit.ridge,xvar = "lambda", label = TRUE)
```

We can see how coefficients are penalized as lambda increases.

Now, we need to pick an optimal value of lambda to pick the best model coefficients. Note that there is also **cv.glmnet** function which can perform k-fold cross-validation.

```{r, fig.height= 6}
# Note that default is 10-fold cross-validation
cv.ridge <- cv.glmnet(x,y,alpha = 0)
plot(cv.ridge)
```

Notice that what we are doing here is actually computatonally intense. 

1. We get very fine grids of labda values (we can customize the range of values, otherwise function will define one for us)
2. Using each lambda value, we perform 10-fold cross validation:
- We fit a model using the Sum of Squares convex optimization and obtain cross-validated error (average of 10 model fits)
3. We continue steps 1 and 2 for all fine grids of lambda values in the range.

Finally we can come up with a series of CV errors (MSE in this case since the outcome is continuous) for all fine grid of values of lambda.

The object returned by cv.ridge has a builtin plot where we can observe the Mean Square Error as a function of different values of log(lambda). 

In this example, we notice that cv.MSE is high when lambda is very high, when the model is restricted. When we relax lambda, cv.MSE gets lower and reaches to a minimum. 

The two vertical lines are produced in the plot, the left one marks the model with min MSE and the middle one shows a little more restricted model 1 standard error away from the minimum MSE. 

The numbers at the top of the plot indicate how many predictors remain in the model during the regularization. Note that since we are doing Ridge regression all predictors remained in the model (19 variables + intercept).

In this example, all models between the log(lambda) ~ 3  and log(lambda) ~ 6 appear to have the minimum MSE. When choosing the optimal model, we might prefer to get the more restricted model that is indicated by the second vertical line, where log(lambda) is closer to 8.

# Lasso regression

Recall that the lasso is very similar to ridge, except we are penalizing the abolute values of coefficients.

Note that in this case we need to define alpha =1, which can be omitted since this is the default of the glmnet function:

```{r, fig.height = 6}
# Fit the lasso regression model

fit.lasso <- glmnet(x,y)
plot(fit.lasso, xvar = "lambda", label = TRUE)
```

Note that we have zero variables in the case of lasso, because it is doing both chrinkage and variable selection.

---

Another intresting plot that can be produced, which plots the **% deviance explained (~ similar to: R-squared)** by the model:

```{r, fig.height= 6}
plot(fit.lasso, xvar = "dev", label = TRUE)
```

**This is a very useful plot**, especially in this example it reveals something quite interesting. 

Notice that the regularized model with shrunken coefficients can already explain 40% of the deviance when we have 5 predictors left and they are very close to zero. Strikingly, at the point when we have 12 predictors included in the model, we only added up another 10% deviance explained, but coefficients have grown substantially. This is the point where model got unstable and most likely started overfitting.

Take home lesson for us is to look carefully to this type of plot and try to locate the sweetspot where coefficients are shrunken, model is simplified, yet we are still able to explain a good fraction of the devience. That is likely to be the most stable model that win the bias-variance trade off.


---

Next, we can use cross-validation to pick the optimal model based on the CV.MSE:

```{r}
cv.lasso <- cv.glmnet(x,y)
plot(cv.lasso)
```

Great! Note what we find:

- The minimum cv.error is obtained when there are 15 predictors.

- 1 standard error away from this min cv.MSE corresponds to the point where we have 5 predictors, consistent with what we has seen above.