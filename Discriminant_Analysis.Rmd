---
title: "Discriminant Analysis and KNN for classification"
author: "Ozan Aygun"
date: "6/04/2017"
output: 
   html_document:
        toc: true
        number_sections: true
        depth: 4
        theme: cerulean
        highlight: tango
        df_print: paged
---

# Introduction

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,results = "markup", fig.align = "center", fig.width = 5, fig.height = 4,message=FALSE,warning=FALSE)
```

**Here our goal is to use linear and quadratic discriminant analysis for making predicitons. We will also compare LDA and QDA with the non-parametric KNN approach in order to get some intuition about which approaches might work better in a given situation.** 

___

Some facts about discriminant analysis:

- Remember that in logistic regression we have a route to obtain Pr(Y=y|X=x), which is the probability of Y being a particular class y, given the value of the predictor X = x.

1. The overall strategy in the discriminant analysis is to model the distribution of X in each class separately, then use **Bayes theorem** to flip the things around and obtain **Pr(Y=y|X=x)** which is our primary interest.

2. When we assume **normal (Gaussian) distributions** AND **equal variances** for each class, this approach is called **linear discriminant analysis (LDA)**.

3.  When we assume **normal (Gaussian) distributions** BUT **unequal covariance matrices** for each class, this approach is called **quadratic discriminant analysis (QDA)**

4. If we simplify the covariance matrix and make it diagonal, in other words, if we assume the **conditional independence** of the variables for each class, then this analysis is called **Naive Bayes (NB)**. Note that this assumption is almost always wrong, and we get biased estimates. However, for classification, the only thing we need to know is which class probability is the largest. Therefore, we can tolerate lots of bias, still get a good classification performance.

**LDA is useful when:**
- n is small, 
- K (number of classes) > 2, 
- classes are well separated,
- Gaussian Assumptions are reasonable.

**Naive Bayes is useful when p (number of predictors) is very large. (e.g: 4000 - 5000 features)**

As a general rule of thumb:

- As n gets larger and p (number of predictors or data dimension) gets smaller, the probability of overfitting gets lower and model parameters will be estimated with less volatility. 

When we fit a LDA, we silently produce **Discriminant variables**, these are special linear combinations of the original variables. These variables will have centroids and they essentially classify Y towards the closest centroid.

Note: Bayes rule is quite powerful in classification problems. Particularly when you have a very good representative sample from the population, and X's are approximately normally distributed, then Bayes rule is probably one of the best classifiers you can train.

___

#Basic exploration of the data

In this exercise, we will use the Smarket data set from ISLR package. This data represents daily percentage returns for the S&P 500 stock index between 2001 and 2005.

```{r,fig.width = 10, fig.height = 10}
require(ISLR)
require(MASS)
names(Smarket)
str(Smarket)
summary(Smarket)
library(knitr)
knitr::kable(head(Smarket),align = "c")
plot(Smarket[,-9], col = ifelse(Smarket$Direction == "Up","green","red"), cex = 0.5, pch = 20)
```

Along with other useful functions, MASS package contains the function to perform LDA.

# Fitting the LDA model

Direction is the response variable, we are trying to use returns from the last two days to predict stock market direction.

```{r}
lda.fit <- lda(Direction ~ Lag1 + Lag2, data = Smarket, subset = Year < 2005)
lda.fit

```

- When training the model we are using a subset of the Smarket data, the data that is acquired before 2005. Later we will use the model to make predictions about the 2005.

# Interpreting the output

- **prior:** Remember the Bayes rule, these are the prior probabilities used in the LDA. Since we haven't specified them, the model uses the proportions of the two classes in the training data by default.

- **Group means:** tabular summary of the means for each variable and for each group. Looks like there are differences between groups.

- **Coefficients of linear discriminants:** these are the coefficients of the linear function LDA uses to separate the classes. Since there are only K=2 classes, we will have K-1 LDs and therefore only have LD1. LD1 in turn formed by coefficients for both actual variables in the training set, and these coefficients are estimated from the training data.

# Interpreting the plot(LDA)

Certain functions, like plot() directly work on LDA:

```{r, fig.height= 7}
plot(lda.fit)
```

This plots the **values of the linear discriminant function** separately for both predicted classes. 

From these plots we can already notice that there is not much difference in the response and these two histograms indeed look on top of each other. This further illustrates that these are two difficult classes to separate from each other. 

# Performing predictions using LDA model fit

In order to perform predictions, we only use the data that belongs to 2005. This is our test data set:

```{r}
Smarket.2005 <- subset(Smarket, Year == 2005)
lda.predict <- predict(lda.fit,newdata = Smarket.2005)
#lda.predict[1:5,] # Note that the prediction is not in matrix format
class(lda.predict)
```

Note that the predictions we get from lda model fit using the predict function are returned as **list**. The length of each argument is the same, so they can be easily converted to data frame:

```{r}
lda.predict <- data.frame(lda.predict)
head(lda.predict)
```

# Interpreting the prediction table

The table consists of:

- Row name (matches to original data)
- **class**: the classification returned
- **posterior probabilities** (printed for each class): note that the class prediction label is the one with higher posterior probability.
- **LD1**: the actual value of the linear discriminant function fitted to the data.

# Prepare the confusion martix

We will tabulate the true labels with the predicted labels:

```{r}
# Rows: predicted labels, Columns: True labels
table(lda.predict$class,Smarket.2005$Direction)
# Diagonal : correct predictions
# Off diagonal: mistakes
```

It looks like we correctly predicted 106 of the UPs and 35 of the Downs in the testing set.

**Our average classification accuracy (i.e: correct classification rate) is:**

```{r}
# How many times on average the prediction is equal to the true labels?
mean(lda.predict$class == Smarket.2005$Direction)
```

**Our average mis-classification rate is:**

```{r}
mean(lda.predict$class != Smarket.2005$Direction)
```

# Conclusions

Note that this is a hard classification problem since stock market is often unpredictable. Despite our LDA prediction performed a better job compared to logistic regression we employed for the same problem before. This again demonstrates that every small tune in the model could make a big difference in the prediction accuracy.

# Quadratic discriminant analysis (QDA)

The syntax of fitting qda is the same as lda. qda() function is also found in the MASS library:

```{r}
qda.fit <- qda(Direction ~ Lag1 + Lag2 , data =  Smarket, subset = Year < 2005)
qda.fit
```

Note that in the case of QDA we have a similar type of output, except the LDs we had inthe LDA.

Making predictions:

```{r}
qda.predict <- data.frame(predict(qda.fit,newdata = Smarket.2005))
head(qda.predict)
```

Prediction output is quite similar, too.

Confusion matrix:

```{r}
table(qda.predict$class, Smarket.2005$Direction)
```

Average classification accuracy:

```{r}
mean(qda.predict$class == Smarket.2005$Direction)
```

Fascinating! We have found that the QDA improved the test accuracy to almost 60%. This implies that the true (Bayes) decision baundary is more closely modeled by the quadratic function rather than the linear alternatives LDA and logit. 

# K-nearest neighbors (KNN)

KNN is a non-parametric approach, which surprisingly works well in many classification scenarios. It is a must-have in our prediction toolbox!

KNN could be espacially useful when the assumptions of LDA or QDA are not met, and when there are relatively few predictors (p < 4). Since KNN is non-parametric, it doesn't assume any particular distribution or shape of the boundary that was assumed by Logistic regression, LDA or QDA.

The basic working principle of KNN is: 

for a given X = x to be predicted, 

- find the K number of nearest neighbors in the training data set, 
- look at their majority classification,
- and assign the class label to that category for this prediction.

Given KNN is a completely non-parametric approach, it doesn't make any assumptions about the shape of the decision boundary. 

- When K=1, the decision baoudary is the most flexible
- As K increases, decision boundary becomes less flexible, eventually approaching to linear.

While KNN could be superior in the cases when LDA and QDA assumptions fail, the choice of K has a critical impact on the test accuracy. Often resampling methods such are CV or Bootstrap are used to determine the optimal K, therefore the optimal flexibility of the decision boundary.

Another important fact, given KNN is a non-parametric approach, we can not obtain a "variable importance" table to asses which variables are the most important for a given classification problem. 

**As a rule of thumb we can summarize the choice of these approaches:**

1. When the true decision boundaries are close to linear, then LDA and logistic regression perform well.

2. In the case of moderately non-linear decision boundaries, QDA performs better than LDA and logistic regression.

3. As the decision boundary gets more complicated, non-parametric approaches, such as KNN would be more useful. However, we will need to use resampling approaches, such as CV, to determine the optimal K, threfore the optimal smoothnes oof the decision boundary.

4. **As a general rule, these approaches perform better when n is large, and p is small (p <4). Therefore, if we can reduce the dimensions of the data, this would make these models more useful and stable.**

## class library and knn() function

We need to use the class library to perform KNN classification:

```{r}
library(class)
```

The description of the knn() function:

k-nearest neighbour classification for test set from training set. For each row of the test set, the k nearest (in Euclidean distance) training set vectors are found, and the classification is decided by majority vote, with ties broken at random. If there are ties for the kth nearest vector, all candidates are included in the vote.

Usage (Note it  doesn't take a formula:

**knn(train, test, cl, k = 1, l = 0, prob = FALSE, use.all = TRUE)**

- train: matrix or data frame of training set cases.
- test: matrix or data frame of test set cases. A vector will be interpreted as a row vector for a single case.
- cl: factor of true classifications of training set
- k: number of nearest neighbours classification considered.

Once again we will use our stock market example to study KNN:

```{r}
attach(Smarket) # This makes the variables available in your environment
```

## Prepare training and test sets for simultaneous use in knn()

Before we use the KNN, we need to prepare the test and training matrices as needed by the knn() function:

```{r}
Xlag <- cbind(Lag1,Lag2)
```

Define the training observations in the form of a boolean vector:

```{r}
train = Year < 2005
```

## Fit the knn model (K =1) and simultaneously make predictions

```{r}
knn.predict <- knn(train = Xlag[train,], test =  Xlag[!train,], 
                   cl= Direction[train], k=1) 
```

**Note that this time we directly obtain the predition vector from the knn() function, since it takes both test and training sets as an argument and returns the prediction labels.**

## Confusion matrix and accuracy

We can determine the prediction accuracy:

```{r}
table(knn.predict, Direction[!train])
mean(knn.predict == Direction[!train])
```

Note that in this particular problem, KNN with K =1 doesn't perform as good as logit, LDA or QDA. The test accuracy is equal to the level of random guessing (50%).

## Revise the model K = 3

What happens if we made the KNN decision boundary less flexible by seting K = 3 ?

```{r}
knn.predict.k3 <- knn(train = Xlag[train,], test =  Xlag[!train,], 
                   cl= Direction[train], k= 3) 
table(knn.predict.k3, Direction[!train])
mean(knn.predict.k3 == Direction[!train])
```

Great! We notice that the overall test accuracy is increased as we made the model more flexible!

## Choosing the optimal K for test accuracy

Let's consider visualizing the test accuracy as we change the K in the knn fit:


```{r}
set.seed(12345)
knn.predict.accuracy <- NULL
for(i in 1:100){
knn.predict.test <- knn(train = Xlag[train,], test =  Xlag[!train,], 
                   cl= Direction[train], k= i) 
knn.predict.accuracy[i] = mean(knn.predict.test == Direction[!train]) 
}
max.accuracy <- max(knn.predict.accuracy)
plot(1:100, knn.predict.accuracy, pch = 20, col = "navy",xlab = "K", type = "l", main = "KNN Test Set Accuracy")
text(x = which(knn.predict.accuracy == max.accuracy) + 1,
     y = max.accuracy, paste0("K = ", which(knn.predict.accuracy == max.accuracy), "\n Accuracy: ", round(max.accuracy,4) ))
points(1:100, knn.predict.accuracy, pch = 20, col = "navy")
```

Therfore, we have two main conclusions here:

1. Finding the right K optimizes the smoothness of the decision boundary and therfore can have impact on the prediction accuracy.

2. Since the choice of the first point (as well as behaviour in ties) is random, the predictions of KNN is dependent on the random number generation seed. For reproducible results, we need to set the random number seed.